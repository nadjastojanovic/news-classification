{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# News Article Classification\n",
        "\n",
        "**Project**: Feature generation using transformers\n",
        "\n",
        "**Author**: Nada Stojanovic\n",
        "\n",
        "**Task**: Use DistilBERT output to train a classifier using Vowpal Wabbit, in order to sort news articles into one of 24 categories, and compare performance to a classifier trained directly on vectorized data.\n",
        "\n",
        "---\n",
        "\n",
        "Transformers are deep neural network models that are designed to process sequential data, such as time series, images, and audio signals. They differ from traditional recurrent and convolutional neural networks in their ability to selectively attend to different parts of the input sequence, allowing them to capture long-range **dependencies** and **interactions** between different elements of the sequence, also known as the principle of **self-attention**.\n",
        "\n",
        "BERT is a **transformer-based** model and an open source machine learning framework for natural language processing. It was designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish **context**. More details in the DistilBERT section below, but some of this contextual data is stored in CLS tokens which are added to every input sequence passed to BERT.\n",
        "\n",
        "In this notebook, I will be extracting output vectors and hidden state embedding information associated with CLS tokens corresponding to short descriptions of news articles, and use them, alongside existing features to predict the category of each  article.\n",
        "\n",
        "## | Preparing the data\n",
        "\n",
        "I will be using a Kaggle dataset found [here]('https://www.kaggle.com/datasets/rmisra/news-category-dataset'). It contains information about nearly 210,000 HuffPost news headlines from 2012 to 2022. Each record in the dataset consists of the following attributes: **category**, **headline**, authors, link, **short_description** and date. To avoid prolonged training times when training BERT, I will only be using a subset of the dataset, namely the first 2,500 rows."
      ],
      "metadata": {
        "id": "fvj1vqQLkdwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Downloading the dataset from Kaggle\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download rmisra/news-category-dataset\n",
        "! unzip news-category-dataset.zip\n",
        "\n",
        "# Reading the file into a dataframe\n",
        "df = pd.read_json('News_Category_Dataset_v3.json', lines = True)"
      ],
      "metadata": {
        "id": "iIPOaNgTnoK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 26 unique categories of news articles represented in the first 2500 instances, which we will be looking to predict based on the article's short description, headline and BERT output. We will not be using the 'authors', 'link' and 'date' columns."
      ],
      "metadata": {
        "id": "NkOZTg9-mhbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing unnecessary columns\n",
        "df = df.drop(['link', 'authors', 'date'], axis=1)\n",
        "# Factorizing the category column\n",
        "df['category'] = pd.factorize(df['category'])[0]\n",
        "# Using a subset of the dataset\n",
        "df = df.iloc[:2500]"
      ],
      "metadata": {
        "id": "bCSSGtunopFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## | Feature Extraction using DistilBERT\n",
        "\n",
        "I will be using **DistilBERT**, a compressed version of BERT. It uses a smaller architecture and fewer parameters than BERT, making it computationally lighter and faster to train and deploy. Despite its smaller size, DistilBERT achieves comparable performance to BERT, preserving over 95% of its performance as measured on the GLUE language understanding benchmark.\n",
        "\n",
        "DistilBERT takes as input a sequence of words, and passes them through a stack of encoders which output a sequence of vectors corresponding to each token of the input sequence, including special tokens such as CLS and SEP. \n",
        "\n",
        "The **CLS token** is of particular interest for this task. It is added to the start of each input sequence, and contains **high-level information** about the sequence as a whole. The output vector corresponding to the CLS token can be used for **classification** and allows the model to make predictions based on the overall meaning of the input, rather than just local context.\n",
        "\n",
        "A graphical representation in which we view DistilBERT as a black box may help visualize this process:\n",
        "\n",
        "        [CLS]  word1  word2  word3  word4 ...    \n",
        "          ↓      ↓      ↓      ↓      ↓       \n",
        "      |------------------------------------|\n",
        "      |             DistilBERT             |\n",
        "      |------------------------------------|      \n",
        "          ↓ \n",
        "    output vector\n",
        "          ↓\n",
        "    |------------|\n",
        "    | Classifier |        ...\n",
        "    |------------|\n",
        "          ↓\n",
        "    Classification\n",
        "\n",
        "I will be using a pre-defined tokenizer and a pre-trained model from DistilBERT's base, imported from HuggingFace."
      ],
      "metadata": {
        "id": "XY_Cn_deUaS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the model and tokenizer from HuggingFace\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Use GPU if possible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-defined DistilBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# Load the pre-trained DistilBert model\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "oVX9xb2Rqclu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will then split the dataset for training and testing, and use the pre-loaded tokenizer on the short description values."
      ],
      "metadata": {
        "id": "D0uZeyPwzC-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and testing tests, with 20% allocated for testing\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize short description values, with applied padding and truncation to ensure\n",
        "# consistent sequence lengths, and return PyTorch tensors\n",
        "tokenized_train = tokenizer(train_df[\"short_description\"].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n",
        "tokenized_test = tokenizer(test_df[\"short_description\"].values.tolist() , padding = True, truncation = True,  return_tensors=\"pt\")\n",
        "\n",
        "# Move to device\n",
        "tokenized_train = {k:v.clone().detach().to(device) for k,v in tokenized_train.items()}\n",
        "tokenized_test = {k:v.clone().detach().to(device) for k,v in tokenized_test.items()}"
      ],
      "metadata": {
        "id": "6AU-I6sHrQSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will then pass the tokenized data to DistilBERT and extract the hidden state corresponding to the CLS token at the start of each input sequence."
      ],
      "metadata": {
        "id": "fSdUM8_kzRBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable gradient calculation to save memory and time\n",
        "with torch.no_grad():\n",
        "  # Generate hidden states using the tokenized data from the previous step as input\n",
        "  hidden_train = model(**tokenized_train)\n",
        "  hidden_test = model(**tokenized_test)\n",
        "\n",
        "# Obtain the hidden state embedding information associated with the CLS token\n",
        "cls_train = hidden_train.last_hidden_state[:,0,:]\n",
        "cls_test = hidden_test.last_hidden_state[:,0,:]\n",
        "\n",
        "# Moving to CPU\n",
        "x_train = cls_train.to(\"cpu\")\n",
        "y_train = train_df[\"category\"]\n",
        "\n",
        "x_test = cls_test.to(\"cpu\")\n",
        "y_test = test_df[\"category\"]"
      ],
      "metadata": {
        "id": "mgy2q_CNsrlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## | Training a Classifier with DistilBERT output\n",
        "\n",
        "I chose to train a classifier with a logisic loss function in VW, although other options could work well too."
      ],
      "metadata": {
        "id": "aZlCNuA7k3zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vowpalwabbit\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vw = vowpalwabbit.Workspace(\"--loss_function hinge --oaa 24\")\n",
        "\n",
        "# Define VW format\n",
        "def to_vw_format(x1, x2, y, cls):\n",
        "    # Flatten the DistilBERT output tensor \n",
        "    cls_str = ' '.join(map(str, cls.flatten().tolist()))\n",
        "    res = f\"{int(y)} | headline: {x1} | description: {x2}| cls: {cls_str}\"\n",
        "    return res\n",
        "\n",
        "# Learn from the training set\n",
        "for x1, x2, y, cls in zip(train_df['headline'], train_df['short_description'], train_df['category'], x_train):\n",
        "    instance = to_vw_format(x1, x2, y, cls)\n",
        "    vw.learn(instance)\n",
        "\n",
        "# Make predictions from the test set\n",
        "predictions = []\n",
        "for x1, x2, y, cls in zip(test_df['headline'], test_df['short_description'], test_df['category'], x_test):\n",
        "    instance = to_vw_format(x1, x2, y, cls)\n",
        "    predicted_class = vw.predict(instance)\n",
        "    predictions.append(predicted_class)"
      ],
      "metadata": {
        "id": "7lS5W2sR-7Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to gain a more comprehensive understanding of the classifier's performance, I opted to compute accuracy, precision, recall and F1 score."
      ],
      "metadata": {
        "id": "OUGE5AgBaYTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = len(y_test[y_test == predictions]) / len(y_test)\n",
        "print(f\"Model accuracy {accuracy:.2f}\")\n",
        "\n",
        "# Evaluate the precision, recall and F1 score of the classifier\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, predictions, average='weighted', zero_division=1)\n",
        "\n",
        "print(f\"Model precision: {precision:.2f}\")\n",
        "print(f\"Model recall: {recall:.2f}\")\n",
        "print(f\"Model F1-score: {f1_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XASILoPmIvOg",
        "outputId": "7df72658-bcdc-4ea3-f259-4fb9e490d54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy 0.32\n",
            "Model precision: 0.40\n",
            "Model recall: 0.32\n",
            "Model F1-score: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## | Training a classifier on original data\n",
        "Rather than comparing the model trained above to a dummy classifier, I thought it would only be fair to compare its performance to that of a classifier trained directly on original data, without the CLS token information."
      ],
      "metadata": {
        "id": "7tga83gQkvgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vw = vowpalwabbit.Workspace(\"--loss_function hinge --oaa 24\")\n",
        "\n",
        "# Define VW format\n",
        "def to_vw_format(x1, x2, y):\n",
        "    res = f\"{int(y)} | headline: {x1} | description: {x2}\"\n",
        "    return res\n",
        "\n",
        "# Learn from the training set\n",
        "for x1, x2, y in zip(train_df['headline'], train_df['short_description'], train_df['category']):\n",
        "    instance = to_vw_format(x1, x2, y)\n",
        "    vw.learn(instance)\n",
        "\n",
        "# Make predictions from the test set\n",
        "predictions = []\n",
        "for x1, x2, y in zip(test_df['headline'], test_df['short_description'], test_df['category']):\n",
        "    instance = to_vw_format(x1, x2, y)\n",
        "    predicted_class = vw.predict(instance)\n",
        "    predictions.append(predicted_class)"
      ],
      "metadata": {
        "id": "HVEw75JSDPoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = len(test_df[test_df.category == predictions]) / len(test_df)\n",
        "print(f\"Model accuracy {accuracy:.2f}\")\n",
        "\n",
        "# Evaluate the precision, recall and F1 score of the classifier\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_df.category, predictions, average='weighted', zero_division=1)\n",
        "\n",
        "print(f\"Model precision: {precision:.2f}\")\n",
        "print(f\"Model recall: {recall:.2f}\")\n",
        "print(f\"Model F1-score: {f1_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRxeE9cJIwyv",
        "outputId": "86740649-16c9-4cc3-ea27-5436ab748e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy 0.62\n",
            "Model precision: 0.63\n",
            "Model recall: 0.62\n",
            "Model F1-score: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## | Discussion\n",
        "\n",
        "Although my expectation was that, at least in some regard, the first classifier would outperform the second one, I was wrong. The first model predicted article category with an accuracy of 32%, precision of 40%, recall of 32% and F1 score of 0.26. The second model predicted article category with an accuracy of 62%, precision of 63%, recall of 62% and F1 score of 0.58.\n",
        "\n",
        "On the basis of these results, it can be concluded that the second approach significantly **outperforms** the first, across the board. However, I theorize that this may not be a reflection on the value of DistilBERT's encoding.\n",
        "\n",
        "In fact, I believe that the results would be different had VW been equipped with native support for DistilBERT, and been able to handle its output accordingly. Furthermore, VW and its classifiers are optimized for larger datasets whereas we are only dealing with 2500 instances.\n",
        "\n",
        "## | Further experimentation\n",
        "\n",
        "In order to test my hypothesis, I will conduct an additional experiment. I will train a Support Vector Machine model from the sklearn library, first on DistilBERT's output, and then just on original data.\n",
        "\n",
        "The reason I opted for SVMs, although other classifiers would work well too, is that they have been shown to perform well in text classification tasks. They are also known to perform well on smaller datasets, which is fitting in this case."
      ],
      "metadata": {
        "id": "b133P6J2l1fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Traing a linear SVM model on DistilBERT's output\n",
        "svm = LinearSVC(random_state=42, max_iter=10000)\n",
        "svm.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = svm.predict(x_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate the precision of the classifier\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Evaluate the recall of the classifier\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# Evaluate the F1 score of the classifier\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f'F1-score: {f1:.2f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70aRuPBgdAWv",
        "outputId": "c07b1ac7-7cbc-42ee-9764-07c19f44f278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.64\n",
            "Precision: 0.66\n",
            "Recall: 0.64\n",
            "F1-score: 0.63\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, I will train the same classifier model on vectorized original data, without DistilBERT's output."
      ],
      "metadata": {
        "id": "Mj54tQyqdTHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(train_df['short_description'])\n",
        "X_test_vect = vectorizer.transform(test_df['short_description'])\n",
        "\n",
        "Y_train_vect = train_df['category']\n",
        "Y_test_vect = test_df['category']\n",
        "\n",
        "# Traing a linear SVM model\n",
        "svm = LinearSVC(random_state=42, max_iter=10000)\n",
        "svm.fit(X_train_vect, Y_train_vect)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = svm.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(Y_test_vect, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Evaluate the precision of the classifier\n",
        "precision = precision_score(Y_test_vect, y_pred, average='weighted', zero_division=1)\n",
        "print(f'Precision: {precision:.2f}')\n",
        "\n",
        "# Evaluate the recall of the classifier\n",
        "recall = recall_score(Y_test_vect, y_pred, average='weighted', zero_division=1)\n",
        "print(f'Recall: {recall:.2f}')\n",
        "\n",
        "# Evaluate the F1 score of the classifier\n",
        "f1 = f1_score(Y_test_vect, y_pred, average='weighted')\n",
        "print(f'F1-score: {f1:.2f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfAE7wzVda0_",
        "outputId": "36b05ba3-061c-4e66-9e74-819a926fb63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.51\n",
            "Precision: 0.53\n",
            "Recall: 0.51\n",
            "F1-score: 0.48\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## | Conclusion\n",
        "\n",
        "Now, the first model predicted article category with an accuracy of 64%, precision of 66%, recall of 64% and F1 score of 0.63. The second model predicted article category with an accuracy of 51%, precision of 53%, recall of 51% and F1 score of 0.88.\n",
        "\n",
        "This confirms that there is some value in DistilBERT's encoding, and affirms its ability to selectively attend to different parts of the input, thus capturing long-range dependencies and interactions between different elements of the sequence.\n",
        "\n",
        "The model has shown it's able to learn which parts of the input sequence are most relevant for the task at hand, and to selectively focus on those parts while ignoring irrelevant information. Translating this concept to the area of feature generation and feature engineering would be extremely valuable.\n",
        "\n",
        "However, as shown by the results of the first set of experiments, further investigations are required to determine the robustness of these results. I've outlined some of the ideas I have for continuing this research below.\n",
        "\n",
        "## | Further research\n",
        "* Experiment with different transformer-based models, such as RoBERTa or GPT-3\n",
        "* Investigate the impact of using larger datasets on model performance, especially in VW\n",
        "* Explore the use of other classifiers, such as Neural Networks, and see if they yield better performance\n",
        "* Experiment with hyperparameter tuning\n",
        "* Experiment with different input data types\n",
        "* Investigate the potential for transfer learning by training the model on one dataset and fine-tuning it on a related dataset\n",
        "\n",
        "## | Citations\n",
        "1. Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).\n",
        "2. Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021)."
      ],
      "metadata": {
        "id": "qmXdGQ4Wc_mj"
      }
    }
  ]
}